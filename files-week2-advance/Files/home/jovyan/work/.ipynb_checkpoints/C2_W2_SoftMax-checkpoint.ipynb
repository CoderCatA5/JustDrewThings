{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Optional Lab - Softmax Function\n",
    "\n",
    "In this lab we will explore the softmax function. This function is used in both Softmax Regression and in Neural Networks when solving Multiclass Classification problems.  \n",
    "\n",
    "<center>  <img  src=\"./images/C2_W2_Softmax.png\" width=\"300\" /><img src=\"./images/C2_W2_SoftMaxNN.png\" width=\"300\" />  <center/>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from lab_utils_multiclass import *\n",
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib widget\n",
    "from matplotlib.widgets import Slider\n",
    "from lab_utils_common import dlc\n",
    "from lab_utils_softmax import plt_softmax\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Softmax Function\n",
    "In both softmax regression and a neural network, N outputs are generated and one output is selected as the predicted answer. In both cases a vector $\\mathbf{z}$ is generated by a linear function (specifically, without a sigmoid) which is fed into a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add up to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities.\n",
    "<center>  <img  src=\"./images/C2_W2_SoftmaxReg_NN.png\" width=\"600\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=0}^{N-1}{e^{z_k} }} \\tag{1}$$\n",
    "The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 0 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N-1 | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=0}^{N-1}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_0} \\\\\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N-1}} \\\\\n",
    "\\end{bmatrix} \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "Let's create a NumPy implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):\n",
    "    ez = np.exp(z)              #element-wise exponenial\n",
    "    sm = ez/np.sum(ez)\n",
    "    return(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, vary the values of the `z` inputs. Note in particular how the exponential in the numerator magnifies small differences in the values. Note as well that the output values sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690f1aaa372542a8b974a8a6c4b26e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "plt_softmax(my_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Numerical Stability\n",
    "The input's  to the softmax are the outputs of a linear layer $z_j = \\mathbf{w_j} \\cdot \\mathbf{x}^{(i)}+b$. This may be a large number. The first step of the softmax algorithm computes $e^{z_j}$. This can result in an overflow error if the number gets too large. Try running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^{500} = 1.40e+217\n",
      "e^{600} = 3.77e+260\n",
      "e^{700} = 1.01e+304\n",
      "e^{800} = inf\n"
     ]
    }
   ],
   "source": [
    "for z in [500,600,700,800]:\n",
    "    ez = np.exp(z)\n",
    "    zs = \"{\" + f\"{z}\" + \"}\"\n",
    "    print(f\"e^{zs} = {ez:0.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation will generate an overflow if the exponent gets too large. Naturally, `my_softmax()` will generate the same errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., nan]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_tmp = np.array([[500,600,700,800]])\n",
    "my_softmax(z_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical stability can be improved by reducing the size of the exponent as shown in (2). If you are interested in the details, click on the details below.\n",
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for details</b></font></summary>\n",
    "Recall \n",
    "$$ e^{a + b} = e^ae^b$$\n",
    "if the $b$  were the opposite sign if $a$, this would reduce the size of the exponent. Specifically, if you multiplied the softmax by a fraction like this:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{i=1}^{N}{e^{z_i} }} \\frac{e^{-b}}{ {e^{-b}}}$$\n",
    "that would not change the value of the softmax. If $b$ in $e^b$ were the largest value of the $z_j$'s, $max(\\mathbf{z})$, the exponent would be reduced to its smallest value.\n",
    "$$\\begin{align}\n",
    "a_j &= \\frac{e^{z_j}}{ \\sum_{i=1}^{N}{e^{z_i} }} \\frac{e^{-max(\\mathbf{z})}}{ {e^{-max_j(\\mathbf{z})}}} \\\\\n",
    "&= \\frac{e^{z_j-max_j(\\mathbf{z})}}{ \\sum_{i=1}^{N}{e^{z_i-max(\\mathbf{z})} }} \n",
    "\\end{align}$$\n",
    "It is customary to say $C=max(\\mathbf{z})$ since the equation would be correct with any constant C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_j = \\frac{e^{z_j-C}}{ \\sum_{k=1}^{N}{e^{z_k-C} }} \\quad\\quad\\text{where}\\quad C=max(\\mathbf{z})\\tag{3}\n",
    "$$\n",
    "\n",
    "If we look at our troublesome example where $\\mathbf{z}$ contains 500,600,700,800, $C=max(\\mathbf{z})=800$:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\frac{1}{ e^{500-800} + e^{600-800} + e^{700-800} + e^{800-800}}\n",
    "\\begin{bmatrix}\n",
    "e^{500-800} \\\\\n",
    "e^{600-800} \\\\\n",
    "e^{700-800} \\\\\n",
    "e^{800-800} \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "5.15e-131 \\\\\n",
    "1.38e-87 \\\\\n",
    "3.7e-44 \\\\\n",
    "1.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite `my_softmax` to improve its numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax_ns(z):\n",
    "    \"\"\"numerically stablility improved\"\"\"\n",
    "    bigz = np.max(z)\n",
    "    ez = np.exp(z-bigz)              # minimize exponent\n",
    "    sm = ez/np.sum(ez)\n",
    "    return(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this and compare it to the tensorflow implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.15e-131 1.38e-087 3.72e-044 1.00e+000] \n",
      " [5.15e-131 1.38e-087 3.72e-044 1.00e+000]\n"
     ]
    }
   ],
   "source": [
    "z_tmp = np.array([500.,600,700,800])\n",
    "print( tf.nn.softmax(z_tmp).numpy(), \"\\n\", my_softmax_ns(z_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now large values no longer cause an overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cost\n",
    "To implement gradient descent, a cost/loss is associated with the softmax outputs.\n",
    "<center> <img  src=\"./images/C2_W2_SoftMaxCost.png\" width=\"400\" />    <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function associated with SoftMax is:\n",
    "\\begin{equation}\n",
    "  L(\\mathbf{a})=\\begin{cases}\n",
    "    -log(a_0), & \\text{if $y=0$}.\\\\\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "Where y is the target category for this example.\n",
    ">**Recall:** Loss is for one example while Cost covers all examples. \n",
    " \n",
    " \n",
    "Note in (4) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target. \n",
    "    $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "    1, & \\text{if $y==n$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "  \n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = - \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "Where $m$ is the number of examples, $N$ is the number of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a case where the target is two ($y=2$) and just look at the loss for that case. This will result in the loss being:  \n",
    "$$L(\\mathbf{a})= -log(a_2)$$\n",
    "Recall that $a_2$ is the output of the softmax function described above, so this can be written:\n",
    "$$L(\\mathbf{z})= -log(\\frac{e^{z_2}}{ \\sum_{k=1}^{N}{e^{z_k} }}) \\tag{6}$$\n",
    "As (5) is taking log's of exponents, is clear there are some numerical optimizations possible. To make those optimizations (*and this is the key point*), the softmax and the loss must be calculated together. This will have an impact on how you write your Tensorflow models. If you are interested in the details of the optimization, open the hint below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for details of optimization</b></font></summary>\n",
    "Starting from (1) above, the loss for the case of y=2:\n",
    "$log(\\frac{a}{b}) = log(a) - log(b)$, so this can be rewritten:\n",
    "$$L(\\mathbf{z})= -\\left[log(e^{z_2}) - log \\sum_{i=1}^{N}{e^{z_i} }\\right] \\tag{6}$$\n",
    "The first term can be simplified to just $z_2$:\n",
    "$$L(\\mathbf{z})= -\\left[z_2 - log( \\sum_{i=1}^{N}{e^{z_i} })\\right] =  \\underbrace{log \\sum_{i=1}^{N}{e^{z_i} }}_\\text{logsumexp()} -z_2 \\tag{7}$$\n",
    "It turns out that $log \\sum_{i=1}^{N}{e^{z_i} }$ term in the above equation is so often used many libraries have an implementation. In Tensorflow this is tf.math.reduce_logsumexp(). As in softmax, an issue with this sum is that the exponent in the sum could overflow if a $z_i$ were large. To fix this, we might like to subtract $e^{max_j(\\mathbf{z})}$ as we did above, but this will require a bit of work:\n",
    "$$\n",
    "\\begin{align}\n",
    "   log \\sum_{i=1}^{N}{e^{z_i} } &= log \\sum_{i=1}^{N}{e^{(z_i - max_j(\\mathbf{z}) + max_j(\\mathbf{z}))}} \\tag{8}\\\\\n",
    "                          &= log \\sum_{i=1}^{N}{e^{(z_i - max_j(\\mathbf{z}))} e^{max_j(\\mathbf{z})}} \\\\\n",
    "                          &= log(e^{max_j(\\mathbf{z})}) + log \\sum_{i=1}^{N}{e^{(z_i - max_j(\\mathbf{z}))}} \\\\\n",
    "                          &= max_j(\\mathbf{z})  + log \\sum_{i=1}^{N}{e^{(z_i - max_j(\\mathbf{z}))}}\n",
    "\\end{align}\n",
    "$$\n",
    "Now, the exponential is less likely to overflow. It is customary to say $C=max_j(\\mathbf{z})$ since the equation would be correct with any constant C. We can now write the loss equation:\n",
    "    \n",
    "$$L(\\mathbf{z})= C+ log( \\sum_{i=1}^{N}{e^{z_i-C} }) -z_2  \\;\\;\\;\\text{where } C=max_j(\\mathbf{z}) \\tag{9} $$\n",
    "The above is for an example where the target, y==2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "The point of describing the optimizations above was to motivate the adaptations in construction a model that allow the softmax and loss functions to be calculated together. Let's create a dataset to train a multiclass classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make  dataset for example\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model below is implemented with the softmax as an activation in the final Dense layer.\n",
    "The loss function is separately specified in the `compile` directive. \n",
    ">** Tensorflow does not differentiate between a single and multiple example loss and referrs to both as 'loss'\n",
    "\n",
    "The loss function `SparseCategoricalCrossentropy`. This is the same as the cost function defined in (5) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output and cannot perform the optimizations for numerical stability described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 986us/step - loss: 1.2107\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 966us/step - loss: 0.4284\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1990\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1192\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0862\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 972us/step - loss: 0.0697\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0603\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0541\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0497\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 950us/step - loss: 0.0465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f00e19a10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'softmax') \n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preferred organization is shown below. In the final layer, there is no activation. The outputs in this form are referred to as *logits* for historical reasons. This corresponds to $\\mathbf{z}$ in the equations above. \n",
    "The loss function has an additional input `from_logits = True`. This informs the loss function that the softmax operation should be included in the loss calculation. This allows for the optimizations described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.8008\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3774\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2230\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1316\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0906\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0716\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0611\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0548\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0503\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 974us/step - loss: 0.0465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f00c2c310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferred_model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'linear')   #<-- Note\n",
    "    ]\n",
    ")\n",
    "preferred_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "preferred_model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Handling**\n",
    "Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction.   \n",
    "Lets look at the **non-preferred model**. The softmax is in the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.75e-04 1.90e-03 9.60e-01 3.67e-02]\n",
      " [9.93e-01 6.55e-03 5.32e-04 1.73e-05]]\n",
      "largest value 0.99999356 smallest value 6.954848e-15\n"
     ]
    }
   ],
   "source": [
    "p_nonpreferred = model.predict(X_train)\n",
    "print(p_nonpreferred [:2])\n",
    "print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the **preferred model** outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.78 -4.19  2.64 -0.38]\n",
      " [ 6.82  1.9  -3.64 -6.75]]\n",
      "largest value 12.5548115 smallest value -10.745525\n"
     ]
    }
   ],
   "source": [
    "p_preferred = preferred_model.predict(X_train)\n",
    "print(p_preferred [:2])\n",
    "print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output predictions are not probabilities!\n",
    "To fix these up, we need to apply a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.13e-02 1.03e-03 9.41e-01 4.62e-02]\n",
      " [9.93e-01 7.31e-03 2.85e-05 1.27e-06]], shape=(2, 4), dtype=float32)\n",
      "largest value 0.99999905 smallest value 2.2022914e-10\n"
     ]
    }
   ],
   "source": [
    "sm_preferred = tf.nn.softmax(p_preferred)\n",
    "print(sm_preferred [:2])\n",
    "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseCategorialCrossentropy or CategoricalCrossEntropy\n",
    "Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.\n",
    "- SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9. \n",
    "- CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "In this lab you \n",
    "- Became more familiar with the softmax function and its use in softmax regression and in softmax activations in neural networks. \n",
    "- Learned the preferred model construction in Tensorflow:\n",
    "    - No activation on final layer (same as linear activation)\n",
    "    - SparseCategoricalCrossentropy loss function\n",
    "    - use from_logits=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_tmp = np.array([797.,798,799,800])\n",
    "#np.logaddexp(z_tmp)\n",
    "tf.math.reduce_logsumexp(z_tmp)\n",
    "zmax = np.max(z_tmp)\n",
    "zr = z_tmp - zmax\n",
    "zmax + np.log(np.sum(np.exp(z_tmp - zmax)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
